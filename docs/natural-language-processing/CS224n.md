# Stanford CS224N: Natural Language Processing with Deep Learning

## 课程简介

- **先修要求**：Python，微积分，线性代数，概率论，机器学习
- **主要内容**：
    - NLP 基本概念，如 word vector, (sub)word model, prompting, pretraining 等
    - 几种常见网络，如 RNN (GRU, LSTM), attention, transformer, BERT 等
    - 一些 NLP 的应用场景，例如 dependency parsing, machine translation, natural language generation 等

这门课相对较基础，课程内容不算太多，不过足够带领你入门自然语言处理。个人认为有价值的部分在 5 个 assignments 和最后的那个 project 上。在 winter 2023 的课程中：

- Assignment 1：初步接触 word vector 的概念，形象化展示 word vector 在空间中的 embedding。
- Assignment 2：实现神经网络中一些简单的小部件，并训练一个 word2vec 模型。如果你早已熟悉，可以跳过这部分。
- Assignment 3：实现一个简单的 neural dependency parser。
- Assignment 4：基于 Seq2Seq 模型（LSTM + attention），实现一个汉译英的 neural machine translator，你会学习到 encoder、decoder 这种基本的架构。由于这个模型中涉及到诸多变量，你可能需要在充分熟悉模型架构后才能顺利完成。
- Assignment 5：实现一个 minGPT 模型，这个作业没有太多提示，大部分都要靠你思考完成。你会分析多头注意力机制的好处，比较模型在有/无 pretraining 时效果的巨大差异。
- Project：实现一个 minBERT 模型，进而利用 pretraining 来完成如 sentiment analysis、paraphrase detection、semantic textual similarity 等下游任务。你可能会感叹一个 BERT 便能将原本需要分别设计并训练若干个不同的模型，转化为“训练——微调”一个任务。

不过，如果你想深入学习当下最火的大语言模型（LLM）的话，这门课可能并不能满足你，还需要继续扩展新的课程。

## 相关链接

1. 课程网站：<https://web.stanford.edu/class/cs224n/>
2. 课程视频：[Youtube](https://www.youtube.com/watch?v=rmVRLeJRkl4&list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4)，[BiliBili](https://www.bilibili.com/video/BV18Y411p79k?vd_source=c9e11661823ca4062db1ef99f7e0eee1)
